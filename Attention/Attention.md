# ä¸è®²5å¾·çš„attentionåˆ°åº•æ˜¯å•¥?

attentionç”±æ¥å·²ä¹…ï¼Œè®©å®ƒåå£°å¤§å™ªçš„è¿˜æ˜¯BERTï¼Œå¯ä»¥è¯´NLPä¸­ï¼ŒBERTä¹‹åï¼Œå†æ— RNNå’ŒCNNã€‚é‚£ä¹ˆattentionåˆ°åº•æœ‰å“ªäº›å‘¢ï¼Ÿhard attentionã€soft attentionã€global attentionã€local attentionã€self-attention, å•Šï¼Œè¿™äº›éƒ½æ˜¯å•¥ï¼Ÿç›¸ä¼¼åº¦è®¡ç®—çš„dotã€generalã€concatéƒ½æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ


# èµ·æº

[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
æ–‡ç« æ˜¯ä¸€ä¸ªå›¾åƒçš„æè¿°ç”Ÿæˆï¼Œencoderåˆ©ç”¨CNNä»å›¾åƒä¸­æå–ä¿¡æ¯ä¸ºLä¸ªDç»´çš„çŸ©é˜µï¼Œä½œä¸ºdecoderçš„è¾“å…¥ã€‚å³:
$$
a_{i}, i=1,2, \ldots, L
$$


è€Œdecoderä½¿ç”¨äº†lstmï¼Œè®¤ä¸ºLä¸ªDç»´å‘é‡å¯ä»¥æ„æˆä¸€ä¸ªåºåˆ—ï¼ˆå¹¶éæ—¶é—´åºåˆ—ï¼Œè€Œä¸”ç©ºé—´ä¸Šçš„åºåˆ—ï¼‰ï¼Œè¿™é‡ŒLSTMä¸­è¾“å‡ºä¸ºä¸Šä¸€æ­¥çš„éšå˜é‡$h_{t-1}$, ä¸Šä¸€æ­¥çš„è¾“å‡º$y_{t-1}$å’Œ$Z_t$, éšå˜é‡å’Œè¾“å‡ºæ˜¯LSMTå·²æœ‰çš„ï¼Œé‚£ä¹ˆ$Z_t$æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Œæ€ä¹ˆè·å¾—çš„å‘¢ï¼Ÿ

è¿™é‡Œä½œè€…æå‡ºäº†attentionæœºåˆ¶ï¼Œæœºå™¨è‡ªåŠ¨å­¦ä¹ è·å¾—attentionæƒé‡åå¯¹å‘é‡åŠ æƒæ±‚å’Œï¼Œè·å¾—$Z_t$ï¼Œå¾ˆæŠ½è±¡ï¼Œå’±ä»¬ç›´æ¥ä¸Šå…¬å¼ï¼š
$$
e_{ti} = f_{att}(a_i, h_{i-1}) = act\_fuc( W^a \times a_i + W^h \times h_{i-1} + b ) \\
\alpha_{t i}=\frac{\exp \left(e_{t i}\right)}{\sum_{k=1}^{L} \exp \left(e_{t k}\right)}
$$
è¿™é‡Œçš„æƒé‡è·å¾—ä½¿ç”¨äº†æ„ŸçŸ¥æœºç»“æ„ï¼Œact_fucæ˜¯æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥æ˜¯sigmoidã€reluç­‰ï¼Œé‚£ä¹ˆ$Z_t$è®¡ç®—ä¸ºattentionæƒé‡çš„åŠ æƒæ±‚å’Œï¼š
$$
Z_t = \sum_{i=1}^L \alpha_{ti} * a_i
$$

æœ¬æ–‡è¿˜æå‡ºæ¥hard/soft attention.

## é‚£ä¹ˆä»€ä¹ˆæ˜¯hard attentionå‘¢ï¼Ÿ

å¯¹äºtæ—¶åˆ»ï¼Œå…¶å¯¹äºä½ç½®içš„attentionçš„æƒé‡è®°ä¸º$S_{ti}$ï¼Œä½œè€…è®¤ä¸º$S_{ti}$åº”è¯¥æœä»å¤šå…ƒä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå³æ‰€æœ‰$s_{ti}, i=1,2,...L$ä¸­åªæœ‰ä¸ªä¸º1ï¼Œæ˜¯ä¸€ä¸ªone-hotå‘é‡ï¼Œå³ï¼š
$$
\begin{array}{l}p\left(s_{t, i}=1 \mid s_{j<t}, \mathbf{a}\right)=\alpha_{t, i} \\ \hat{\mathbf{z}}_{t}=\sum_{i} s_{t, i} \mathbf{a}_{i}\end{array}
$$

å¯ä»¥çœ‹åˆ°$\alpha_{t,i}$æ²¡æœ‰ç›´æ¥å‚ä¸$\hat z_t$çš„è®¡ç®—ï¼ŒæŸå¤±å‡½æ•°å½“ç„¶æ˜¯åœ¨æ¡ä»¶açš„æƒ…å†µä¸‹æœ€å¤§åŒ–æ­£ç¡®yçš„æ¦‚ç‡ï¼Œå³$\log p(y|a)$ï¼Œä½œè€…é€šè¿‡Jensen ä¸ç­‰å¼å°†ç›®æ ‡å‡½æ•°å®šä¹‰ä¸º$\log p(y|a)$çš„ä¸€ä¸ªä¸‹ç•Œï¼Œå·§å¦™çš„å°†såŠ å…¥åˆ°æŸå¤±å‡½æ•°ä¸­ï¼š
$$
\begin{aligned} L_{s} &=\sum_{s} p(s \mid \mathbf{a}) \log p(\mathbf{y} \mid s, \mathbf{a}) \\ & \leq \log \sum_{s} p(s \mid \mathbf{a}) p(\mathbf{y} \mid s, \mathbf{a}) \\ &=\log p(\mathbf{y} \mid \mathbf{a}) \end{aligned}
$$
è®¡ç®—æ¢¯åº¦ï¼š
$$
\begin{aligned} \frac{\partial L_{s}}{\partial W}=\sum_{s} p(s \mid \mathbf{a}) &\left[\frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W}+\right.\left.\log p(\mathbf{y} \mid s, \mathbf{a}) \frac{\partial \log p(s \mid \mathbf{a})}{\partial W}\right] \end{aligned}
$$


æŸå¤±å‡½æ•°ä¼¼ä¹æ²¡æ³•æ±‚å¯¼ï¼Œé‚£æ€ä¹ˆåŠå‘¢ï¼Ÿå¯¹ï¼Œéšæœºé‡‡æ ·ã€‚åˆ©ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•å¯¹ s è¿›è¡ŒæŠ½æ ·ï¼Œæˆ‘ä»¬åš N æ¬¡è¿™æ ·çš„æŠ½æ ·å®éªŒï¼Œè®°æ¯æ¬¡å–åˆ°çš„åºåˆ—æ˜¯æ˜¯$\tilde{s}^{n}$ï¼Œå…¶æ¦‚ç‡å°±æ˜¯1/Nï¼Œé‚£ä¹ˆæ¢¯åº¦ç»“æœï¼š
$$
\begin{aligned} \frac{\partial L_{s}}{\partial W} \approx \frac{1}{N} \sum_{n=1}^{N}\left[\frac{\partial \log p\left(\mathbf{y} \mid \tilde{s}^{n}, \mathbf{a}\right)}{\partial W}+\right. \left.\log p\left(\mathbf{y} \mid \tilde{s}^{n}, \mathbf{a}\right) \frac{\partial \log p\left(\tilde{s}^{n} \mid \mathbf{a}\right)}{\partial W}\right] \end{aligned}
$$


## soft attention

ç›¸å¯¹è€Œè¨€soft attentionå°±å®¹æ˜“ç†è§£ï¼Œç›¸æ¯”äºone-hot, sotfå³å…¨éƒ¨ä½ç½®éƒ½ä¼šæœ‰åŠ å…¥ï¼ŒåŒºåˆ«åœ¨äºæƒé‡çš„å¤§å°ï¼Œæ­¤æ—¶ï¼š
$$
Z_t = \beta_t * \sum_{i=1}^L \alpha_{ti} * a_i \\ \beta_{t}=\sigma\left(f_{\beta}\left(h_{t-1}\right)\right)
$$
å…¶ä¸­

åŒæ—¶ï¼Œæ¨¡å‹æŸå¤±ä¸­åŠ å…¥äº†$\alpha_{ti}$çš„æ­£åˆ™é¡¹ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ
$$
L_{d}=-\log (P(\mathbf{y} \mid \mathbf{x}))+\lambda \sum_{i}^{L}\left(1-\sum_{t}^{C} \alpha_{t i}\right)^{2}
$$
é¦–å…ˆattentionæƒé‡è¿›è¿‡sotfmaxæ˜¯ä¿è¯$\sum_i^L \alpha_{ti} = 1$ï¼ŒåŒæ—¶æ­¤é¡¹æŸå¤±ä¸­çš„æ­£åˆ™ä¿è¯$\sum_t^C \alpha_{ti} \approx 1$ï¼Œå…¶ä¸­$\sum_t^C \alpha_{ti}$è¡¨ç¤ºåŒä¸€ä¸ªtçš„æ‰€æœ‰è¢«å…³æ³¨(attention)çš„æƒé‡å’Œï¼Œæ‰€ä»¥è¦æ±‚æ¯ä¸ªä½ç½®è¢«é‡è§†çš„æ€»å’Œç›¸ç­‰ã€‚

> è¿™ä¸ªâ€œè¢«â€å­—æœ‰ç‚¹ç»•ï¼Œä¸¾ä¾‹i=1æ—¶å€™$\alpha_{11}$è¡¨ç¤º$a_1$å¯¹äº$a_1$çš„æƒé‡ï¼Œ$\alpha_{21}$è¡¨ç¤º$a_1$å¯¹äº$a_2$çš„æƒé‡ï¼Œä»¥æ­¤ç±»æ¨ï¼Œ$\alpha_{L1}$è¡¨ç¤º$a_1$å¯¹äº$a_L$çš„æƒé‡ï¼Œ$\sum_t^C \alpha_{ti}$=1ï¼Œå³è¦æ±‚$a_1, a_2,...a_L$åœ¨è¢«attentionçš„æ€»å’Œç›¸ç­‰ï¼Œä¿è¯æ¯ä¸ªéƒ¨ä½è¢«åŒæ ·å…³æ³¨ã€‚

é‚£ä¹ˆä»€ä¹ˆåˆæ˜¯global attention å’Œ local attentionå‘¢ï¼Ÿ
# global attention å’Œ local attention

æ˜¯å¦æ˜¯è¯´æœ‰äº›éƒ¨åˆ†çš„attentionå¹¶ä¸ç”¨å…³æ³¨äºå…¨å±€çš„ä¿¡æ¯ï¼Œåªéœ€è¦å…³æ³¨éƒ¨åˆ†çš„ä¿¡æ¯å°±å¥½äº†ï¼Œ é‚£ä¹ˆæ˜¯å¦å¯ä»¥æœ‰attentionåªå…³æ³¨ä¸€éƒ¨åˆ†ä½ç½®ä¸Šçš„è¾“å‡ºå‘¢ï¼Ÿ

> [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)

[Effective](https://arxiv.org/abs/1508.04025)æå‡ºäº†global attention å’Œ local attentionæ¦‚å¿µï¼Œå…·ä½“å¯ä»¥çœ‹å›¾

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20201228003656741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaW5lMTk5MzA4MjA=,size_16,color_FFFFFF,t_70)![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20201228003705391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaW5lMTk5MzA4MjA=,size_16,color_FFFFFF,t_70)

> å›¾ä¸­å·¦è¾¹ä¸ºå…¨å±€attentionï¼Œå³è¾¹ä¸ºlocalã€‚è“è‰²å—è¡¨ç¤ºè¾“å…¥åºåˆ—ï¼Œçº¢è‰²å—è¡¨ç¤ºç”Ÿæˆåºåˆ—ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œglobalåœ¨ç”Ÿæˆ$c_t$æ—¶å€™å›è€ƒè™‘å…¨å±€çš„è¾“å…¥ï¼Œå’Œæ­£å¸¸attentionæ— å¼‚ã€‚
>
> local attentionä¼šæœ‰ä¸€ä¸ªçª—å£ï¼Œåœ¨çª—å£ä¸­çš„è¾“å…¥æ‰ä¼šè¢«è®¡ç®—æƒé‡ï¼Œå¯ä»¥è®¤ä¸ºå…¶ä½™éƒ½æ˜¯0ã€‚è¿™è®©æˆ‘æƒ³åˆ°äº†å·ç§¯ğŸ¤£
>
> æœ€ç»ˆçš„ä¼šå°†äºŒè€…çš„contextå‘é‡å’Œ$h_t$ concatä½œä¸ºæœ€ç»ˆçš„è¾“å‡ºã€‚

**global attention:** å¯¹äºglobal attentionï¼Œå…¶è¾“å…¥åºåˆ—$\bar{h}_{s}, s=1,2, \ldots, n$, å¯¹äºè¾“å‡ºåºåˆ—$h_t$ï¼Œå’Œæ¯ä¸ª$\bar{h}_{s}$è®¡ç®—attentionæƒé‡ç„¶ååŠ æƒæ±‚å’Œè·å¾—contextå‘é‡, attentionæƒé‡è®¡ç®—æ–¹å¼ä¸ºï¼š
$$
\alpha_t(s)=\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s^{\prime}}\right)\right)} \tag{7}
$$
é‚£ä¹ˆå…¶ä¸­çš„scoreæ˜¯æ€ä¹ˆè®¡ç®—çš„å‘¢ï¼Œä½œè€…æ€»ç»“äº†ä¸€ä¸‹å†å²çš„attentionçš„æƒé‡3ç§è®¡ç®—æ–¹å¼ï¼š
$$
\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{ll}\boldsymbol{h}_{t}^{\top} \overline{\boldsymbol{h}}_{s} & \text { dot } \\ \boldsymbol{h}_{t}^{\top} \boldsymbol{W}_{\boldsymbol{a}} \overline{\boldsymbol{h}}_{s} & \text { general } \\ \boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{a}}\left[\boldsymbol{h}_{t} ; \overline{\boldsymbol{h}}_{s}\right]\right) & \text { concat }\end{array}\right.
$$

> å…¶å®åŒ…æ‹¬åé¢çš„transformerã€bertç­‰ï¼Œéƒ½æ˜¯éµå¾ªæ­¤èŒƒå¼ï¼Œä¸è¿‡æ˜¯scoreè®¡ç®—æ–¹å¼åœ¨dotåŸºç¡€ä¸Šé™¤ä»¥å‘é‡ç»´åº¦çš„0.5æ¬¡æ–¹ï¼Œä¸ºäº†æ¶ˆé™¤ç»´åº¦å¯¹scoreçš„å½±å“ã€‚

**local attention:** æ¯æ¬¡éƒ½è®¡ç®—å…¨å±€çš„attentionæƒé‡ï¼Œè®¡ç®—å¼€é”€ä¼šç‰¹åˆ«å¤§ï¼Œç‰¹åˆ«æ˜¯è¾“å…¥åºåˆ—å¾ˆé•¿çš„æ—¶å€™ï¼ˆä¾‹å¦‚ä¸€ç¯‡æ–‡æ¡£ï¼‰ï¼Œæ‰€ä»¥æå‡ºäº†æ¯æ¬¡å€¼å…³æ³¨ä¸€å°éƒ¨åˆ†positionã€‚é‚£ä¹ˆæ€ä¹ˆç¡®å®šè¿™ä¸€å°éƒ¨åˆ†å‘¢ï¼Ÿ

æ–‡ä¸­è®¾å®šäº†ä¸€ä¸ªcontextå‘é‡$c_t$åªå…³æ³¨å…¶çª—å£$[p_t-D, p_t+D]$å†…çš„haidden statesï¼Œè€Œ$p_t$æ€ä¹ˆæ¥çš„å‘¢ï¼Œæ–‡ä¸­åˆå®šä¹‰äº†è¿™ä¹ˆå‡ ç§æ–¹å¼ï¼š

- Monotonic alignmentï¼š$P_t=t$, è¿™æ˜¾ç„¶ä¸å¤ªåˆé€‚ç¿»è¯‘ï¼Œé™¤éæ˜¯alignmentçš„ä»»åŠ¡ï¼Œä¾‹å¦‚åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œå…¶æ ‡ç­¾å’Œå½“å‰tå¼ºç›¸å…³ã€‚
- Predictive alignmentï¼š$p_{t}=S \cdot \operatorname{sigmoid}\left(\boldsymbol{v}_{p}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{p}} \boldsymbol{h}_{t}\right)\right)$, é€šè¿‡è®¡ç®—è·å¾—ï¼Œå–å†³äºè¾“å…¥$h_t$ï¼Œå³$h_t$ favor alignment pointsï¼ˆä¸çŸ¥é“å’‹ç¿»è¯‘ï¼ŒGç‚¹å§ï¼‰ï¼Œæ‰€ä»¥monotonicè‚¯å®šæ˜¯alignmentä»»åŠ¡æ‰åˆé€‚ã€‚

ç„¶åæƒé‡è®¡ç®—æ–¹å¼ä¸ºï¼š
$$
\boldsymbol{a}_{t}(s)=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \exp \left(-\frac{\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right)
$$

> å¯èƒ½ç»†å¿ƒçš„è§‚ä¼—è¦é—®ï¼Œalignæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿå¥½å§ï¼Œè‡ªå·±çœ‹å…¬å¼7.

å¯ä»¥çœ‹åˆ°ï¼Œåœ¨æ™®é€šçš„æƒé‡è®¡ç®—åŸºç¡€ä¸Šï¼ŒåŠ å…¥äº†ä¸€ä¸ªè·ç¦»çš„å½±å“å› å­ï¼Œè·ç¦»è¶Šå°ï¼Œåé¢ä¸€é¡¹è¶Šå¤§ï¼Œè¯´æ˜æ­¤æ›´å€¾å‘äºä¸­å¿ƒä½ç½®åˆ°æƒé‡å¤§ï¼Œè¶Šè¿œä½ç½®è¶Šè¾¹ç¼˜ï¼Œç”šè‡³è¶…è¿‡è¾¹ç¼˜å°±è¢«è£æ‰ï¼ˆä¾‹å¦‚çª—å£å¤–çš„å°±ä¸º0ï¼‰

æ€»ç»“ä¸‹æ¥local attentionå…³æ³¨éƒ¨åˆ†positionï¼Œè€Œglobal attentionå…³æ³¨å…¨å±€çš„positionã€‚

# Scaled Dot-Product Attention

> Transformerä¸­attentionå¾¡ç”¨æ–¹å¼ã€‚ç”¨Transformerå®Œå…¨æ›¿ä»£äº†RNNç»“æ„ã€‚
>
> [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
> [Weighted Transformer Network for Machine Translation](https://arxiv.org/abs/1711.02132)

ä¸è®²5å¾·ï¼Œç›´æ¥ä¸Šå…¬å¼ï¼Œ
$$
\text {Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \\ = \operatorname{softmax}\left(\left[\begin{array}{c}v_{1} \\ v_{2} \\ \cdots \\ v_{n}\end{array}\right] *\left[v_{1}^{T}, v_{2}^{T}, \ldots, v_{n}^{T}\right]\right) *\left[\begin{array}{c}v_{1} \\ v_{2} \\ \ldots \\ v_{n}\end{array}\right]
$$
å…¶ä¸­ï¼Œ$v_i$è¡¨ç¤ºæ¯ä¸€æ­¥çš„tokençš„å‘é‡ï¼Œåœ¨self attentionä¸­ï¼ŒQ,K,Væ¥æºäºåŒä¸€ä¸ªè¾“å…¥Xï¼š
$$
Q_i=X_i \times W^q  \\
K_i=X_i \times W^k  \\
V_i=X_i \times W^v
$$

å¯ä»¥çœ‹åˆ°ï¼Œå’Œä¹‹å‰attentionè®¡ç®—æ–¹å¼å·®å¼‚å¹¶ä¸å¤§ï¼Œåˆ†æ¯å¤šäº†ä¸€é¡¹$\sqrt{d_{k}}$æ˜¯ä¸ºäº†æ¶ˆé™¤ç»´åº¦å¯¹äºattentionçš„å½±å“ã€‚

åŒæ—¶è¿˜æå‡ºäº†å¤šå¤´æœºåˆ¶ï¼ˆmulti-head attentionï¼‰ï¼Œæœ‰ç‚¹ç±»ä¼¼äºCNNä¸­çš„å·ç§¯æ ¸æ•°ç›®ã€‚

![image-20210107233327919](/Users/zhiyang.zzy/project/py3project/MyPicture/Attention/image-20210107233327919.png)

**multi-head attention**ï¼šç”±å¤šä¸ªscaled dot-product attentionç»„æˆï¼Œè¾“å‡ºç»“æœconcatï¼Œæ¯ä¸€ä¸ªattentionéƒ½éƒ½æœ‰ä¸€å¥—ä¸åŒçš„æƒé‡çŸ©é˜µ$W_{i}^{Q}, W_{i}^{K}, W_{i}^{V}$, ä¼šæœ‰ä¸åŒçš„åˆå§‹åŒ–å€¼ã€‚
$$
\begin{aligned} \operatorname{MultiHead}(Q, K, V) &=\operatorname{Concat}\left(\operatorname{head}_{1}, \ldots, \mathrm{head}_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}
$$

> åŒæ—¶ç”±äºTransformerä¸­è®¾ç½®äº†æ®‹å·®ç½‘ç»œï¼Œè®¾ç½®éšå±‚å•å…ƒæ•°ç›®å’Œå¤´æ•°æ—¶å€™è¦æ³¨æ„æ˜¯å¦æ»¡è¶³ï¼šnum_attention_heads * attention_head_size = hidden_size

åŒæ—¶è¿˜æ˜¯ç”¨position-wise feed-forward networksã€position encodingã€layer normalizationã€residual connectionç­‰ï¼Œç»§ç»­å¡«å‘ï¼Œåç»­ä¹Ÿæœ‰ä¸€äº›å¯¹transformerçš„æ”¹é€ ï¼Œä¼šç»§ç»­æ›´æ–°ã€‚



## Position-wise Feed-Forward Networks



## position encoding

ä»attentionçš„è®¡ç®—ä¸­å¯ä»¥çœ‹å‡ºï¼Œä¸åŒæ—¶åºçš„åºåˆ—è®¡ç®—attentionçš„ç»“æœæ˜¯ä¸€æ ·çš„ï¼Œå¯¼è‡´Transformerä¼šå˜æˆä¸€ä¸ªè¯è¢‹æ¨¡å‹ï¼Œé‚£ä¹ˆæ€ä¹ˆå¼•å…¥åºåˆ—çš„ä¿¡æ¯å‘¢ï¼Ÿæ‰€ä»¥è¿™é‡Œå°±éœ€è¦å¯¹positionè¿›è¡Œè¡¨ç¤ºï¼ŒåŠ åˆ°åŸæœ‰çš„tokenå‘é‡ä¸Šï¼Œè®©æ¯ä¸ªtokenä¸­åŒ…å«ä½ç½®ä¿¡æ¯ï¼Œä¸åŒçš„tokenä¹‹é—´åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œé‚£ä¹ˆæ€ä¹ˆè¡¨ç¤ºè¿™ç§ç»å¯¹å’Œç›¸å¯¹çš„ä½ç½®ä¿¡æ¯å‘¢ï¼Ÿ

è®ºæ–‡ä¸­position encodingä½¿ç”¨äº†å…¬å¼:
$$
\begin{aligned} P E_{(p o s, 2 i)} &=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\ P E_{(p o s, 2 i+1)} &=\cos \left(\text { pos } / 10000^{2 i / d_{\text {model }}}\right) \end{aligned}
$$

> å¹¶ä¸”è®ºæ–‡è¯•éªŒäº†ä½¿ç”¨åŸºäºè®­ç»ƒçš„position embeddingæ–¹å¼ï¼Œå‘ç°æ•ˆæœå·®åˆ«ä¸å¤§ï¼Œè€Œä¸Šé¢æ–¹å¼ä¼˜åŠ¿åœ¨äºä¸éœ€è¦è®­ç»ƒï¼Œå‡å°‘äº†è®¡ç®—é‡ã€‚

ä½†æ˜¯çœ‹åç»­bertæºç ä¸­ä»ç„¶ä½¿ç”¨position embeddingçš„æ–¹å¼ï¼Œå³æ¯ä¸ªpositionéšæœºåˆå§‹åŒ–ä¸€ä¸ªå‘é‡ï¼Œé€šè¿‡å’Œæ¨¡å‹ä¸€èµ·è®­ç»ƒæ¥æ‹Ÿåˆæœ€ç»ˆçš„positionå‘é‡ã€‚

ä»£ç è§ï¼š[attention.py](https://github.com/InsaneLife/MyPicture/blob/master/Attention/attention.py)

# Reference

- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Weighted Transformer Network for Machine Translation](https://arxiv.org/abs/1711.02132)
- https://www.zhihu.com/question/68482809/answer/1574319286
- https://zhuanlan.zhihu.com/p/47282410
- https://www.jiqizhixin.com/articles/2018-06-11-16
- https://github.com/JayParks/transformer

